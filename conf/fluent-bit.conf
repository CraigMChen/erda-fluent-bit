[SERVICE]
    # Flush
    # =====
    # set an interval of seconds before to flush records to a destination
    flush        0.5

    # Daemon
    # ======
    # instruct Fluent Bit to run in foreground or background mode.
    daemon       Off

    grace 1
    # Log_Level
    # =========
    # Set the verbosity level of the service, values can be:
    #
    # - error
    # - warning
    # - info
    # - debug
    # - trace
    #
    # by default 'info' is set, that means it includes 'error' and 'warning'.
    log_level    info

    # Parsers File
    # ============
    # specify an optional 'Parsers' configuration file
    parsers_file parsers.conf

    # Plugins File
    # ============
    # specify an optional 'Plugins' configuration file to load external plugins.
    plugins_file plugins.conf

    # HTTP Server
    # ===========
    # Enable/Disable the built-in HTTP Server for metrics
    http_server  On
    http_listen  0.0.0.0
    http_port    2020

    # Storage
    # =======
    # Fluent Bit can use memory and filesystem buffering based mechanisms
    #
    # - https://docs.fluentbit.io/manual/administration/buffering-and-storage
    #
    # storage metrics
    # ---------------
    # publish storage pipeline metrics in '/api/v1/storage'. The metrics are
    # exported only if the 'http_server' option is enabled.
    #
    #storage.metrics on

    # storage.path
    # ------------
    # absolute file system path to store filesystem data buffers (chunks).
    #
    #storage.path /data/spot/fluentbit/storage

    # storage.sync
    # ------------
    # configure the synchronization mode used to store the data into the
    # filesystem. It can take the values normal or full.
    #
    # storage.sync normal

    # storage.checksum
    # ----------------
    # enable the data integrity check when writing and reading data from the
    # filesystem. The storage layer uses the CRC32 algorithm.
    #
    # storage.checksum off

    # storage.backlog.mem_limit
    # -------------------------
    # if storage.path is set, Fluent Bit will look for data chunks that were
    # not delivered and are still in the storage layer, these are called
    # backlog data. This option configure a hint of maximum value of memory
    # to use when processing these records.
    #
    #storage.backlog.mem_limit 20M

[INPUT]
    name                 tail
    path                 /var/log/containers/*.log
    DB                   /data/spot/fluent-bit/flb_kube.db
    Tag                  kube.*
    read_from_head       false
    Buffer_Chunk_Size    100k
    Buffer_Max_Size      1MB
    Skip_Long_Lines      On
    multiline.parser     docker
    Refresh_Interval     20
    Rotate_Wait          600
    Offset_Key           offset

    Mem_Buf_Limit        100MB
    # storage.type         filesystem


[FILTER]
    name                    multiline
    match                   kube.*
    multiline.key_content   log
    multiline.parser        default-multiline-regex

[FILTER]
    Name                kubernetes
    Match               kube.*
    Kube_URL            ${MASTER_VIP_URL}
    Kube_CA_File        /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
    Kube_Token_File     /var/run/secrets/kubernetes.io/serviceaccount/token
    Kube_Tag_Prefix     kube.var.log.containers.
    Merge_Log           On
    Merge_Log_Key       log_processed
    K8S-Logging.Parser  On
    K8S-Logging.Exclude Off

[OUTPUT]
    name stdout
    match xx
    format json

[OUTPUT]
    name                      erda
    match                     *
    Retry_Limit               False

    # remote config
    erda_ingest_url           ${OUTPUT_ERDA_INGEST_URL}
    # extract included environment from container, and then add into tags
    container_env_include     TERMINUS_DEFINE_TAG,TERMINUS_KEY,MESOS_TASK_ID,DICE_ORG_ID,DICE_ORG_NAME,DICE_PROJECT_ID,DICE_PROJECT_NAME,DICE_APPLICATION_ID,DICE_APPLICATION_NAME,DICE_RUNTIME_ID,DICE_RUNTIME_NAME,DICE_SERVICE_NAME,DICE_WORKSPACE,DICE_COMPONENT,TERMINUS_LOG_KEY,MONITOR_LOG_KEY,DICE_CLUSTER_NAME,MSP_ENV_ID,MSP_LOG_ATTACH,POD_IP
    # container is a job when it has environment TERMINUS_DEFINE_TAG
    # gzip enable when compress level > 0
    compress_level            3
    # HTTP request timeout
    request_timeout           20s
    keepalive_idle_timeout    30s
    # if basic_auth_username or basic_auth_password, then we use basic auth
    basic_auth_username ${COLLECTOR_AUTH_USERNAME}
    basic_auth_password ${COLLECTOR_AUTH_PASSWORD}

    # batch flush config
    batch_trigger_timeout     ${OUTPUT_BATCH_TRIGGER_TIMEOUT}
    batch_event_content_limit_bytes  ${OUTPUT_BATCH_TRIGGER_CONTENT_LIMIT_BYTES}

